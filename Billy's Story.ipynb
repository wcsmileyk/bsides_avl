{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1: Colecting security data**\n",
    "\n",
    "Billy has determined this is not an area worth building any tools yet. He’s going to install a linux distro on one of his allocated systems. It comes with rsyslog:\n",
    "\n",
    "- Configuration is incredibly simple\n",
    "- Except for his Windows servers, all his security data can be forwarded via syslog\n",
    "- Free\n",
    "- Problem Solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can simply consist of uncommenting the following lines in `/etc/rsyslog.conf` on almost every linux system:\n",
    "\n",
    "```\n",
    "$ModLoad imudp\n",
    "$UDPServerRun 514\n",
    "```\n",
    "\n",
    "Billy chose a pretty simple configuration. He recommends reviwing the rsylog docs if doing this yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note on Windows events:</b> \n",
    "    \n",
    "Billy knows this will be an issue in the future. And has some plans in mind to solve it. But he’s ignoring those for now. Because I only have 50 minutes to tell you his story.\n",
    "\n",
    "- Windows events are not stored in plain text, so take a bit more effort to access\n",
    "- There are 3rd party/commercial agents that can forward syslog. He might look into this. It’s the easiest solution.\n",
    "-If he decides he can’t afford those he can always use a python library called “pywin” to remotely grab the events from his newly built syslog server\n",
    "- For now, all security data is collected by a Windows Event Collector\n",
    "- Priority is on Firewall and VPN data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2: Parsing and storing data**\n",
    "\n",
    "- Each log currently has its own format\n",
    "- He needs to have a simple way to correlate and create rules to analyze the data\n",
    "- Probably wants some indexes to make querying and rules simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next block of code is just doing some simple file/directory prep before processing the logs and parsing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Billy has hard-coded his base and log directories using a relative path. \n",
    "# Making this more flexible would be a good future improvement\n",
    "\n",
    "import os\n",
    "\n",
    "base_dir = os.path.abspath(os.path.dirname('.'))\n",
    "log_dir = os.path.join(base_dir, 'logs')\n",
    "json_dir = os.path.join(base_dir, 'json')\n",
    "\n",
    "# Billy's initial focus is just vpn and firewall logs\n",
    "log_files = ['openvpnas.log', 'firewall.log']\n",
    "\n",
    "log_paths = [os.path.join(log_dir, f) for f in log_files]\n",
    "\n",
    "# If the director we will use as a pseudo database doesn't exist create it\n",
    "# This should be an area Billy focuses on improving in the future to ensure this script works in other directories\n",
    "if not os.path.isdir(json_dir):\n",
    "    os.mkdir(json_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Billy needs to define the parsing functions for re-use. Doing t his outside a function would get quite messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "def parse_log(log_message):\n",
    "    \"\"\"\n",
    "    Takes the syslog header from any type of syslog messages and builds the intial json used for all parsers\n",
    "    and then attemps to parse further based on the application type\n",
    "    \n",
    "    This will require future updates to ensure application types are added, could be made easier with OOP\n",
    "    \"\"\"\n",
    "    \n",
    "    dt, log_source, log_app, message_body = log_message.split(' ', 3)\n",
    "    log_app_map = {'sshd': 'ssh', 'openvpnas': 'vpn', 'kernel': 'firewall'}\n",
    "\n",
    "    for app_name in log_app_map:\n",
    "        if log_app.startswith(app_name):\n",
    "            app = log_app_map[app_name]\n",
    "            break\n",
    "        else:\n",
    "            app = log_app\n",
    "\n",
    "    log_time = parser.parse(dt)\n",
    "    timestamp = datetime.datetime.timestamp(log_time)\n",
    "    \n",
    "    parsed_syslog = {'log_source_time': timestamp, 'log_source': log_source, 'app': app, 'message': message_body}\n",
    "    \n",
    "    parse_funcs = {'firewall': parse_fw, 'vpn': parse_ovpn}\n",
    "    \n",
    "    # Try to parse the additional fields based on application\n",
    "    if app in parse_funcs:\n",
    "        try:\n",
    "            parse_func = parse_funcs[app]\n",
    "            parsed_message = parse_func(message_body)\n",
    "            for k,v in parsed_message.items():\n",
    "                parsed_syslog[k] = v\n",
    "        except (AttributeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    return parsed_syslog\n",
    "\n",
    "\n",
    "def parse_fw(fw_message):\n",
    "    \"\"\"\n",
    "    Parses IP tables firewalls (or any space seperated key=value formatted event message)\n",
    "    \n",
    "    This method could be a good one for re-use in the future since it isn't so locked to a one off format.\n",
    "    \"\"\"\n",
    "    \n",
    "    msg_dict = {'additional_data': []}\n",
    "    for pair in fw_message.split(' '):\n",
    "        try:\n",
    "            k, v = pair.split('=', 1)\n",
    "            msg_dict[k] = v\n",
    "        except ValueError:\n",
    "            msg_dict['additional_data'].append(pair)\n",
    "    return msg_dict\n",
    "\n",
    "\n",
    "def parse_ovpn(ovpn_message):\n",
    "    \"\"\"\n",
    "    Parses Openvpn formatted log messages. This is a very specific and complicated format. Unlikely to be re-used\n",
    "    and if Billy wants more than just authenticated users and remote IP's he'll need to make adjustments\n",
    "    \"\"\"\n",
    "    \n",
    "    # For now let's just get the IP assignment, that is post success and cleans things up a bit. \n",
    "    # This can be expanded later\n",
    "    if 'primary virtual IP' not in ovpn_message:\n",
    "        return None\n",
    "\n",
    "    # If you aren't familiar with regex. Get familiar, you'll use it a lot in these types of tools\n",
    "    user_pattern = re.compile('(\\w+)/((\\d{1,3}\\.){3}\\d{1,3})')\n",
    "    date_pattern = re.compile('(\\w{3}\\s+\\w{3}\\s+\\d{1,2}\\s+(\\d{1,2}:){2}\\d{1,2}\\s+\\d{4})\\s')\n",
    "\n",
    "    user_ip = user_pattern.search(ovpn_message)\n",
    "    if user_ip:\n",
    "        user = user_ip.group(1)\n",
    "        ip_addr = user_ip.group(2)\n",
    "        date_search = date_pattern.search(ovpn_message)\n",
    "        event_time = parser.parse(date_search.group(1))\n",
    "        event_timestamp = datetime.datetime.timestamp(event_time)\n",
    "\n",
    "        return {'remote_user': user, 'remote_ip': ip_addr, 'event_time': event_timestamp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is Billy's main formating/parsing prep block, calling the `parse_log` method and converting all logs to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "events = []\n",
    "\n",
    "for log_file in log_paths:\n",
    "    \n",
    "    with open(log_file, 'r') as lf:\n",
    "        # Billy doesn't want massive json files to read. So he's going to lock each file down to 5000 events\n",
    "        # This should keep files roughly under 5MBs (based on current logs) and make them easier to read\n",
    "        count = 0\n",
    "        event_block = {}\n",
    "        \n",
    "        for line in lf:\n",
    "            parsed_log = parse_log(line)\n",
    "            \n",
    "            # Billy creates an MD5 value from each log to ensure they have a unique ID\n",
    "            # He could have just incremented by one, but that added complexities and maybe even race conditions\n",
    "            log_hash = hashlib.md5()\n",
    "            log_hash.update(json.dumps(parsed_log).encode('utf-8'))\n",
    "            log_id = log_hash.hexdigest()\n",
    "            \n",
    "            event_block[log_id] = parsed_log\n",
    "            count += 1\n",
    "            \n",
    "            # This is where Billy does his count check.\n",
    "            # He could also do a slightly more complicated size check if he needed to be exact in size\n",
    "            if count == 5000:\n",
    "                events.append(event_block)\n",
    "                event_block = {}\n",
    "                count = 0\n",
    "        \n",
    "        # For that last block that won't get to 5000 events\n",
    "        events.append(event_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the last block for this task. Here, Billy is simply generating a unique file name for each event block and writing a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_block in events:\n",
    "    # Billy is genearting a datetime based filename down to the microsecond.\n",
    "    # This ensure even with very fast processing each file will almost surely have a unique filename.\n",
    "    # He could also do a hashing method, or just incremental names\n",
    "    \n",
    "    dt = datetime.datetime.now()\n",
    "    seconds = (dt.hour + dt.minute) * 60 + dt.second\n",
    "    file_name = f\"{dt.strftime('%Y%m%d')}_{seconds}.{dt.microsecond}\"\n",
    "    file_path = os.path.join(json_dir, file_name)\n",
    "    \n",
    "    # Billy convert's the python dictionary to json and write it to the file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(event_block, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Building correlation rules\n",
    "\n",
    "The SIEM will have a lot of correlation rules out of the box. But Billy already knows a few simple ways to identify user account compromises. He just needs to automate them:\n",
    "\n",
    "- Users often use a VPN to work remotely. \n",
    "- This is a simple way to identify possible account compromises\n",
    "- If a user logs into the VPN from two different countries within a short period of time, that’s probably bad\n",
    "- Most SIEMs will include this rule by default. But Billy thinks he can write the same logic fairly easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Billy needs a VPN index to make this rule a bit faster. He's going to create a user, remote ip, country code and timestamp index\n",
    "\n",
    "This first block will be some house keeping to isolate VPN events for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpn_events = {}\n",
    "\n",
    "# This is where storing events in blocks comes in handy. \n",
    "# Processing 100s of MBs or more of data would be very resource intensive\n",
    "for json_file in os.listdir(json_dir):\n",
    "    fp = os.path.join(json_dir, json_file)\n",
    "    with open(fp, 'r') as jf:\n",
    "        events = json.load(jf)\n",
    "    \n",
    "    # reducing the data to just VPN events will make indexing and correlation faster\n",
    "    for event_id, event in events.items():\n",
    "        if event['app'] == 'vpn':\n",
    "            vpn_events[event_id] = event  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Billy needs to start actually building the indexes\n",
    "\n",
    "This will involve making an API call to enrich each IP with it's geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the only non standard libary module Billy needs for this project\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_ip_country(ip_addr):\n",
    "    \"\"\"\n",
    "    Takes in an IP address and queries it against ip-api.com a geo lookup resource with a free tree.\n",
    "    \"\"\"\n",
    "    url = f'http://ip-api.com/json/{ip_addr}'\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        ip_data = r.json()\n",
    "        return ip_data['countryCode']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now he's use that method and some simple dictionaries to build out a set of indexes. He could right those to a file for future use, but for now he's only working with them in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "    \n",
    "user_index = {}\n",
    "ip_index = {}\n",
    "cc_index = {}\n",
    "time_index = {}\n",
    "\n",
    "\n",
    "# Billy will be using a free API with rate limiting to get geo data. \n",
    "# This makes it important to track his call rate to ensure he doesn't get errors or not data\n",
    "start_time = time.time()\n",
    "call_count = 0\n",
    "\n",
    "for event_id, event in vpn_events.items():\n",
    "    user = event.get('remote_user')\n",
    "    r_ip = event.get('remote_ip')\n",
    "    \n",
    "    if user:\n",
    "        if not user_index.get(user):\n",
    "            user_index[user] = [event_id]\n",
    "        else:\n",
    "            user_index[user].append(event_id)\n",
    "        \n",
    "        \n",
    "    if r_ip:\n",
    "        if not cc_index.get(r_ip):\n",
    "            # checking the current rate\n",
    "            if call_count == 145 and time.time() - start_time <= 60:\n",
    "                time.sleep(60 - (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                call_count = 0\n",
    "            country = get_ip_country(r_ip)\n",
    "            if country:\n",
    "                cc_index[r_ip] = country\n",
    "            call_count += 1\n",
    "        \n",
    "        if not ip_index.get(r_ip):\n",
    "            ip_index[r_ip] = [event_id]\n",
    "        else:\n",
    "            ip_index[r_ip].append(event_id)\n",
    "    \n",
    "    if r_ip or user:\n",
    "        time_index[event_id] = event['event_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now for the actual correlation piece. \n",
    "\n",
    "Don't be intimated if this feels like a lot of code for one thing. Billy is laying the foundation and almost everything here can be modified just slightly to be reusable by lots and lots of logs and rules.\n",
    "\n",
    "This is just his first draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "rule_matches = []\n",
    "\n",
    "# This is a huge area that Billy can clean up in the future.\n",
    "# Creating some functions from this will make it more readable, repeat less code\n",
    "# and make it more reusable in other code.\n",
    "for user in user_index:\n",
    "    matched_events = []\n",
    "    \n",
    "    for event in user_index[user]:\n",
    "        event_time = time_index.get(event)\n",
    "        \n",
    "        for r_ip in ip_index:\n",
    "            if event in ip_index[r_ip]:\n",
    "                remote_ip = r_ip\n",
    "                country = cc_index[r_ip]\n",
    "                \n",
    "    for c_event in user_index[user]:\n",
    "        if c_event != event:\n",
    "            c_event_time = time_index.get(c_event)\n",
    "            \n",
    "            for r_ip in ip_index:\n",
    "                if c_event in ip_index[r_ip]:\n",
    "                    c_remote_ip = r_ip\n",
    "                    c_country = cc_index[r_ip]\n",
    "            \n",
    "            if country != c_country:\n",
    "                if abs(c_event_time - event_time) <= 3600:\n",
    "                    if event not in matched_events:\n",
    "                        matched_events.append(event)\n",
    "                    if c_event not in matched_events:\n",
    "                        matched_events.append(c_event)\n",
    "    \n",
    "    if len(matched_events) > 1 and len(rule_matches) > 0:\n",
    "        for match in rule_matches:\n",
    "            if Counter(match) != Counter(matched_events):\n",
    "                matches.append(matched_events)\n",
    "    elif len(matched_events) > 1:\n",
    "        rule_matches.append(matched_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And finally billy wrote some code to help give him the important information about each suspicious event\n",
    "\n",
    "This is another area that Billy surely will be able to update to a function later. But for now he's just sticking to a simple code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time | user | country code | remote ip\n",
      "--------------------------------------\n",
      "2019-05-09 22:39:31 | smiley | 112.251.21.161 | CN\n",
      "2019-05-09 21:57:04 | smiley | 76.210.33.168 | US\n",
      "2019-05-09 21:59:31 | smiley | 76.210.33.168 | US\n",
      "2019-05-09 22:25:36 | smiley | 94.176.148.227 | RO\n",
      "2019-05-09 22:33:18 | smiley | 178.128.229.53 | CA\n",
      "2019-05-09 22:35:52 | smiley | 71.85.118.117 | US\n"
     ]
    }
   ],
   "source": [
    "headers = 'time | user | country code | remote ip'\n",
    "print(headers)\n",
    "print ('-' * len(headers))\n",
    "\n",
    "for match in rule_matches:\n",
    "    for event_id in match:\n",
    "        for f in os.listdir(json_dir):\n",
    "            fp = os.path.join(json_dir, f)\n",
    "            with open(fp, 'r') as infile:\n",
    "                events = json.load(infile)\n",
    "\n",
    "            if event_id in events:\n",
    "                full_event = events[event_id]\n",
    "                break\n",
    "\n",
    "        event_time = datetime.datetime.fromtimestamp(full_event['event_time']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        user = full_event['remote_user']\n",
    "        ip = full_event['remote_ip']\n",
    "        country_code = cc_index[ip]\n",
    "        \n",
    "        row_string = ' | '.join([event_time, user, ip, country_code])\n",
    "        print(row_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 4: Enrich our events**\n",
    "\n",
    "Billy now can fairly quickly build rules and work with his event data. But he only has internal data, there’s a lot more info out there\n",
    "\n",
    "- Open source and commercial threat intelligence, IOC lists\n",
    "- Basic nslookup and whois data\n",
    "- Port scanning\n",
    "- Billy is pretty sure he can make is VPN rule more valuable in his teams’ investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Billy is going to use an open Threat Intelligence exchange from Alienvault to get started.\n",
    "# He'll be able to modify this as he adds other enrichment feeds in the future\n",
    "\n",
    "def alienvault_ip_lookup(ip_addr):\n",
    "    \n",
    "    # In this case Billy has written his API token to a file to read later.\n",
    "    # He could also have saved it as an environment variable and used os.environ.get('VARIABLENAME')\n",
    "    # NEVER HARD CODE CREDENTIALS IN CODE. THEY WILL END UP IN GIT OR SOMEWHERE ELSE\n",
    "    key_file = os.path.join(base_dir, 'alienvault.key')\n",
    "    with open(key_file, 'r') as kf:\n",
    "        api_token = kf.read()\n",
    "\n",
    "    url = f'https://otx.alienvault.com:443/api/v1/indicators/IPv4/{ip_addr}'\n",
    "    headers = {'X-OTX-API-KEY': api_token, 'Accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        threat = r.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    pulse_info = threat.get('pulse_info')\n",
    "    if pulse_info:\n",
    "        pulses = pulse_info.get('pulses')\n",
    "        tags = [tag for pulse in pulses for tag in pulse.get('tags')]\n",
    "    else:\n",
    "        tags = None\n",
    "\n",
    "    desc = threat['base_indicator'].get('description')\n",
    "    reputation = threat.get('reputation')\n",
    "\n",
    "    return {'description': desc, 'tags': tags, 'reputation': reputation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that he has the ability to get enriched data, it's time to display it. \n",
    "\n",
    "Billy is just going to re-use his previous code with a slight modification. Now he's seeing why a function may have been a good idea earlier\n",
    "\n",
    "He could also store the enriched data somewhere with his events or indexes for future correlations and investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time | user | country code | remote ip | alienv desc | alienv tags | alienv reputation\n",
      "--------------------------------------------------------------------------------------\n",
      "2019-05-09 22:39:31 | smiley | CN | 112.251.21.161 | SSH bruteforce client IP | ['SSH', 'bruteforce', 'honeypot']\n",
      "2019-05-09 21:57:04 | smiley | US | 76.210.33.168 | None | []\n",
      "2019-05-09 21:59:31 | smiley | US | 76.210.33.168 | None | []\n",
      "2019-05-09 22:25:36 | smiley | RO | 94.176.148.227 | None | []\n",
      "2019-05-09 22:33:18 | smiley | CA | 178.128.229.53 | None | []\n",
      "2019-05-09 22:35:52 | smiley | US | 71.85.118.117 | None | []\n"
     ]
    }
   ],
   "source": [
    "headers = 'time | user | country code | remote ip | alienv desc | alienv tags | alienv reputation'\n",
    "print(headers)\n",
    "print ('-' * len(headers))\n",
    "\n",
    "for match in rule_matches:\n",
    "    for event_id in match:\n",
    "        for f in os.listdir(json_dir):\n",
    "            fp = os.path.join(json_dir, f)\n",
    "            with open(fp, 'r') as infile:\n",
    "                events = json.load(infile)\n",
    "\n",
    "            if event_id in events:\n",
    "                full_event = events[event_id]\n",
    "                break\n",
    "\n",
    "        event_time = datetime.datetime.fromtimestamp(full_event['event_time']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        user = full_event['remote_user']\n",
    "        ip = full_event['remote_ip']\n",
    "        country_code = cc_index[ip]\n",
    "        \n",
    "        alv_rep = alienvault_ip_lookup(ip)\n",
    "        \n",
    "        print(f'{event_time} | {user} | {country_code} | {ip} | {alv_rep[\"description\"]} | {alv_rep[\"tags\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 5: Quick vulnerability scan**\n",
    "\n",
    "After Billy finishes investigating the compromised VPN account, he is told there’s a large scale vulnerability in a fairly uncommon application and Billy isn’t sure if the company is vulnerable. But he doesn't want to assume they aren't\n",
    "\n",
    "- Billy knows a string that is in the vuln application’s banner\n",
    "- Billy knows the port the application uses\n",
    "- So, he’s pretty sure he can quickly identify vulnerable hosts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a good opportunity for Billy to invest some time into his engineering skills\n",
    "\n",
    "He's familiar with the concept of threading but has never used it. He does know, that it will make running several scans much quicker. So he takes about an hour before writing his scanning functions to research Threading and write his code in a way that supports it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan on 192.168.1.0/24\n",
      "Starting scan on 10.1.1.0/24\n",
      "Starting scan on 10.0.1.0/24\n",
      "Got a banner from 192.168.1.114 on 9001\n",
      "\n",
      "****192.168.1.114 running vulnerable app****\n",
      "\n",
      "Completed scan on 192.168.1.0/24\n",
      "\n",
      "Completed scan on 10.0.1.0/24\n",
      "Completed scan on 10.1.1.0/24\n",
      "\n",
      "All scans completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import ipaddress\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "app_port = 9001\n",
    "vuln_string = 'Vully the basic chat application'\n",
    "\n",
    "def get_check_banner(target, port, banner_string):\n",
    "    '''\n",
    "    This is the core scan function that will run in a threat handler for each target\n",
    "    '''\n",
    "    try:\n",
    "        s = socket.socket()\n",
    "        s.connect((target, port))\n",
    "        banner_bytes = s.recv(1024)\n",
    "        print(f'Got a banner from {target} on {port}\\n')\n",
    "        banner = banner_bytes.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    if banner:\n",
    "        if banner.startswith(banner_string):\n",
    "            print(f'****{target} running vulnerable app****\\n')\n",
    "   \n",
    "\n",
    "def banner_scan(network, port, banner_string):\n",
    "    targets = [str(ip) for ip in ipaddress.IPv4Network(network)]\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    print(f'Starting scan on {network}')\n",
    "    \n",
    "    for target in targets:\n",
    "        t = Thread(target=get_check_banner, args=(target, port, banner_string))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "        \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    print(f'Completed scan on {network}\\n')\n",
    "\n",
    "\n",
    "networks = ['192.168.1.0/24', '10.1.1.0/24', '10.0.1.0/24']\n",
    "\n",
    "scan_threads = []\n",
    "\n",
    "for network in networks:\n",
    "    n = Thread(target=banner_scan, args=(network, app_port, vuln_string))\n",
    "    scan_threads.append(n)\n",
    "    n.start()\n",
    "    \n",
    "for scan in scan_threads:\n",
    "    n.join()\n",
    "    \n",
    "print('All scans completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
